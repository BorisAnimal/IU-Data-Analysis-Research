{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to use guide https://www.kaggle.com/amirrezaeian/time-series-data-analysis-using-lstm-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ford\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import numpy as np # linear algebra\n",
    "# from scipy.stats import randint\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL\n",
    "import matplotlib.pyplot as plt # this is used for the plot the graph \n",
    "# import seaborn as sns # used for plot interactive graph. \n",
    "# from sklearn.model_selection import train_test_split # to split the data into two parts\n",
    "# from sklearn.cross_validation import KFold # use for cross validation\n",
    "# from sklearn.preprocessing import StandardScaler # for normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "# from sklearn.pipeline import Pipeline # pipeline making\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.feature_selection import SelectFromModel\n",
    "# from sklearn import metrics # for the check the error and accuracy of the model\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "\n",
    "## for Deep-learing:\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD \n",
    "# from keras.callbacks import EarlyStopping\n",
    "# from keras.utils import np_utils\n",
    "import itertools\n",
    "from keras.layers import LSTM\n",
    "# from keras.layers.convolutional import Conv1D\n",
    "# from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "## from REFIT tutorial\n",
    "from lxml import objectify\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r'dataset/REFIT_BUILDING_SURVEY.xml'\n",
    "tree = objectify.parse(path)\n",
    "root = tree.getroot()\n",
    "NS={'a':'http://www.refitsmarthomes.org'}\n",
    "\n",
    "path=r'dataset/REFIT_TIME_SERIES_VALUES.csv'\n",
    "\n",
    "cache = 'csv.pkl'\n",
    "\n",
    "if os.path.isfile(cache):\n",
    "    with open(cache, 'rb') as f:\n",
    "        # The protocol version used is detected automatically, so we do not\n",
    "        # have to specify it.\n",
    "        csv = pickle.load(f)\n",
    "else:\n",
    "    try:\n",
    "        csv\n",
    "    except:\n",
    "        csv=pd.read_csv(path, index_col=0, parse_dates=[1])\n",
    "    with open(cache, 'wb') as f:\n",
    "        # Pickle the 'data' dictionary using the highest protocol available.\n",
    "        pickle.dump(csv, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    \n",
    "    \n",
    "## Global parameters\n",
    "building_num = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildings = list(range(1,19))\n",
    "del buildings[4]\n",
    "del buildings[2]\n",
    "period = 'D'\n",
    "\n",
    "data = []\n",
    "\n",
    "## Get gas meter data\n",
    "appliance    = 1 #Number of gas meter\n",
    "for building_num in buildings:\n",
    "    tsv = root.xpath('./a:Stock/a:Building[%d]/a:Meter[%d]/a:Sensor/a:TimeSeriesVariable' % (building_num, appliance), namespaces=NS)\n",
    "    id = tsv[0].get('id')\n",
    "    gasdf = csv.loc[id]\n",
    "    header = 'gas_' + str(building_num)\n",
    "    data_gas = {header: gasdf['data'].tolist()}\n",
    "    time_gas = gasdf['dateTime'].tolist()\n",
    "    gasdf = pd.DataFrame(data_gas, index=time_gas).resample(period).sum()\n",
    "    data.append(gasdf)\n",
    "\n",
    "## Merge two DataFrames gas meter and weather\n",
    "df = pd.concat(data, axis=1)\n",
    "df.index.name = 'time'\n",
    "    \n",
    "# print(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Get climate data\n",
    "elements=root.xpath('./a:Stock/a:Climate/a:Sensor/a:TimeSeriesVariable', namespaces=NS)\n",
    "data_clim = {}\n",
    "time_clim = []\n",
    "agg = {}\n",
    "period = 'D'\n",
    "\n",
    "for e in elements:\n",
    "    id = e.get('id')\n",
    "    v = csv.loc[id]\n",
    "    sensor_name = e.get('variableType').replace(' ', '_')\n",
    "    if len(time_clim) < 1:\n",
    "        time_clim = v['dateTime'].tolist() \n",
    "    if sensor_name == 'Total_rainfall':\n",
    "        tmp = np.sum\n",
    "    else:\n",
    "        tmp = np.mean\n",
    "    agg[sensor_name] = tmp\n",
    "    data_clim[sensor_name] = v['data'].tolist()\n",
    "climdf = pd.DataFrame(data_clim, index=time_clim).resample(period).agg(agg)\n",
    "##clean climdf (only when not resamplet for Day 'D')\n",
    "#climdf = climdf.iloc[1::2] ## Some timeSteps are not in gas_meter indexes (15 mins and 45 mins values, 00 and 30 mins are in)\n",
    "\n",
    "\n",
    "buildings = list(range(1,21))\n",
    "del buildings[4]\n",
    "del buildings[2]\n",
    "\n",
    "## Get gas meter data\n",
    "appliance    = 1 #Number of gas meter\n",
    "tsv = root.xpath('./a:Stock/a:Building[%d]/a:Meter[%d]/a:Sensor/a:TimeSeriesVariable' % (building_num, appliance), namespaces=NS)\n",
    "id = tsv[0].get('id')\n",
    "gasdf = csv.loc[id]\n",
    "data_gas = {'gas': gasdf['data'].tolist()}\n",
    "time_gas = gasdf['dateTime'].tolist()\n",
    "gasdf = pd.DataFrame(data_gas, index=time_gas).resample(period).sum()\n",
    "agg['gas'] = np.sum\n",
    "\n",
    "## Merge two DataFrames gas meter and weather\n",
    "df = pd.concat([climdf, gasdf], axis=1)\n",
    "df.index.name = 'time'\n",
    "## clean rows\n",
    "df = df[np.isfinite(df['Air_temperature'])]   # We don't need data where no weather forecast\n",
    "lastind = time_gas[-1]\n",
    "df = df[:lastind]                             # We don't need data where no meter values\n",
    "\n",
    "\n",
    "## NaN rows detection\n",
    "j = len(df.columns) - 1 # last column is 'gas'\n",
    "df.iloc[:,j]=df.iloc[:,j].fillna(df.iloc[:,j].mean())\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Wind_speed'].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = list(range(len(df.columns)))\n",
    "i = 1\n",
    "\n",
    "tmp = pd.DataFrame(df.resample('6M').mean())\n",
    "\n",
    "for col in tmp.columns:\n",
    "    tmp[col].plot(kind='bar', color='r', legend=True )\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "## convert illuminance values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "period = 'W' ## M, Q, W, A == Year\n",
    "\n",
    "## resampling over week and computing mean\n",
    "df.Air_temperature.resample(period).plot(color='y', legend=True)\n",
    "# df.Average_barometric_pressure.resample(period).mean().plot(color='r', legend=True)\n",
    "# df.Relative_humidity.resample(period).mean().plot(color='b', legend=True)\n",
    "df.Total_rainfall.resample(period).plot(color='r', legend=True)\n",
    "# df.Wind_speed.resample(period).plot(color='b', legend=True)\n",
    "df.gas.resample(period).plot(color='g', legend=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pltCor(df):\n",
    "    print('\\n'.join(['%d: %s' % (i, j) for i, j in enumerate(df.columns.tolist())]))\n",
    "    # Correlations among columns\n",
    "    plt.matshow(df.corr(method='spearman'),vmax=1,vmin=-1,cmap='PRGn')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "pltCor(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations of mean of features resampled over months\n",
    "\n",
    "print('\\n'.join(['%d: %s' % (i, j) for i, j in enumerate(df.columns.tolist())]))\n",
    "\n",
    "plt.matshow(df.resample('W').mean().corr(method='spearman'),vmax=1,vmin=-1,cmap='PRGn')\n",
    "plt.title('resampled over month', size=15)\n",
    "plt.colorbar()\n",
    "plt.margins(0.02)\n",
    "plt.matshow(df.resample('6M').mean().corr(method='spearman'),vmax=1,vmin=-1,cmap='PRGn')\n",
    "plt.title('resampled over year', size=15)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start to prepare data and build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resample = df.resample('D').agg(agg)\n",
    "df_resample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kqk = df_resample.index.to_period('M').start_time.unique()[1:]\n",
    "# print(kqk)\n",
    "\n",
    "kqk = [df_resample.index.get_loc(i) for i in kqk]\n",
    "kqk.insert(0,0)\n",
    "kqk[-1] = df_resample.shape[0]\n",
    "print('-'.join(map(str,kqk)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    dff = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(dff.shift(i))\n",
    "        names += [(data.columns[j] + ' (t-%d)' % (i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(dff.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [(data.columns[j] + ' (t)') for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [(data.columns[j] + ' (t+%d)' % (i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resample.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(df_resample, 3, 1)\n",
    "\n",
    "\n",
    "# drop columns we don't want to predict\n",
    "predName = 'gas (t)'\n",
    "tmp = reframed.corr(method='spearman')[predName]\n",
    "del tmp[predName]\n",
    "\n",
    "\n",
    "#Select features with relevat correlation\n",
    "corLev = 0.6\n",
    "tmp = tmp.loc[tmp.abs() < corLev].index.tolist()\n",
    "kek = list(map(reframed.columns.get_loc, tmp))\n",
    "reframed.drop(reframed.columns[kek], axis=1, inplace=True)\n",
    "\n",
    "pltCor(reframed)\n",
    "print('Shape is: ', reframed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predNamepredName = 'gas (t)'\n",
    "tmp = reframed.corr(method='spearman')[predName]\n",
    "del tmp[predName]\n",
    "\n",
    "tmp\n",
    "# tmp = tmp.loc[tmp.abs() > 0.5].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scale data to (0..1) values range## Scale \n",
    "values = reframed.values\n",
    "\n",
    "X,Y = values[:, :-1], values[:, -1]\n",
    "scalerX = MinMaxScaler(feature_range=(0, 1))\n",
    "X = scalerX.fit_transform(X)\n",
    "scalerY = MinMaxScaler(feature_range=(0, 1))\n",
    "Y = scalerY.fit_transform(Y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "assert Y.shape[0] == X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = min(17, X.shape[1])\n",
    "\n",
    "pca = PCA(n_components=comp) # PCA(n_components=2) # PCA(0.9)\n",
    "\n",
    "X = pca.fit_transform(X=X) \n",
    "\n",
    "# to get how much variance was retained\n",
    "print(pca.explained_variance_ratio_.sum())\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split into train and test sets\n",
    "n_train_time = int(reframed.shape[0] * 0.67)\n",
    "train_X, train_y = X[:n_train_time, :], Y[:n_train_time, :]\n",
    "test_X, test_y   = X[n_train_time:, :], Y[n_train_time:, :]\n",
    "\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape) \n",
    "# We reshaped the input into the 3D format as expected by LSTMs, namely [samples, timesteps, features]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(Dense(100))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=30, batch_size=7, validation_data=(test_X, test_y), verbose=0, shuffle=False)\n",
    "\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "# invert scaling for forecast\n",
    "inv_yhat = scalerY.inverse_transform(yhat)\n",
    "# invert scaling for actual\n",
    "inv_y = scalerY.inverse_transform(test_y)\n",
    "\n",
    "\n",
    "# calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "\n",
    "interval = 100\n",
    "## time steps, every step is one hour (you can easily convert the time step to the actual time index)\n",
    "## for a demonstration purpose, I only compare the predictions in 200 hours. \n",
    "aa=[x for x in range(interval)]\n",
    "plt.plot(aa, inv_y[:interval], marker='.', label=\"actual\")\n",
    "plt.plot(aa, inv_yhat[:interval], 'r', label=\"prediction\")\n",
    "plt.ylabel('Global_active_power', size=15)\n",
    "plt.xlabel('Time step', size=15)\n",
    "plt.legend(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X.reshape((X.shape[0], 1, X.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "time = []\n",
    "\n",
    "yy = scalerY.inverse_transform(Y)\n",
    "ypred = scalerY.inverse_transform(pred)\n",
    "ypred = np.array([i if i >= 0.0 else 0.0 for i in ypred]).reshape(-1,1)\n",
    "\n",
    "for k in range(len(kqk)-1):\n",
    "    data.append(np.sqrt(mean_squared_error(yy[kqk[k]:kqk[k+1]], ypred[kqk[k]:kqk[k+1]])))\n",
    "    time.append(df_resample.index[kqk[k]])\n",
    "    \n",
    "rmse = np.sqrt(mean_squared_error(yy, ypred))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time, data)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin = 0\n",
    "end = yy.shape[0]\n",
    "\n",
    "aa = list(range(begin, end))\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(aa,ypred[begin:end], marker='.', label=\"prediction\")\n",
    "plt.plot(aa,yy[begin:end],'r', marker='*', label=\"actual\")\n",
    "plt.ylabel('Global_active_power', size=15)\n",
    "plt.xlabel('Time step', size=15)\n",
    "plt.legend(fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "kak = [abs((ypred[i] - yy[i])) for i in range(begin, end)]\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(aa, kak)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shelve\n",
    "# db = shelve.open(\"brute_PCA_dim.pkl\")\n",
    "# db['data'] = data\n",
    "# db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(list(range(30,80,5)), data)\n",
    "# plt.show()\n",
    "\n",
    "# предсказывает отрицательные величины\n",
    "\n",
    "# pca\n",
    "# Автокорреляция\n",
    "# попробовать из статейки\n",
    "# predict season\n",
    "# Other topology (LSTM at least)\n",
    "# Добавить день недели, месяц, число, часы в инпуты\n",
    "\n",
    "#Предсказывать потребление для района, нахуй, а не для одной семьи/дома!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## to release gpu\n",
    "\n",
    "# def free_gpu():\n",
    "#     try:\n",
    "#         del model\n",
    "#     except:\n",
    "#         print('No model')\n",
    "#     try:\n",
    "#         del history\n",
    "#     except:\n",
    "#         print('No history')\n",
    "#     import gc\n",
    "#     K.clear_session()\n",
    "#     for i in range(15): gc.collect()\n",
    "\n",
    "# free_gpu()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
